{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f615467",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "(1) Create tensors from list, numpy array.   \n",
    "(2) Change datatype.   \n",
    "(3) Reshape.   \n",
    "(4) Mathematical operations.    \n",
    "(5) Concatenate, stack, split, reduce dimension.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfa2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f197189",
   "metadata": {},
   "source": [
    "## Create tensor from list or numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07968ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a list\n",
    "a = [1,2,3]\n",
    "t_a = torch.tensor(a)\n",
    "print(t_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7d16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# The default type is int64\n",
    "print(t_a.dtype)\n",
    "print(type(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ade72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a numpy array\n",
    "b = np.array([4,5,6])\n",
    "t_b = torch.tensor(b)\n",
    "print(t_b)\n",
    "# Note that the type is int32!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6d599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7696a",
   "metadata": {},
   "source": [
    "The default element type after convertion from a list and from an array are different. The reason is the difference between the two sources. The default type of the integer elements in the list 'a' is int64 (This will show as 'int' if we print its type. In fact, this default type depends on platform and python version). On the other hand, the default type of the elements in the numpy array 'b' is int32, at least on this laptop. torch.tensor does not change the data types. It converts the original type to the corresponding type in torch: int64-->torch.int64, int32-->torch.int32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d206df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# We can specify the data type manually\n",
    "# Below, the data type of the tensor is not shown. This is because it has the default data type, which is torch.int64\n",
    "c = np.array([4,5,6], dtype=np.int64)\n",
    "t_c = torch.tensor(c)\n",
    "print(t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e1bd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Tensor whose elements are all 1's\n",
    "t_ones = torch.ones(2,3)\n",
    "print(t_ones)\n",
    "print(t_ones.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477964f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4436, 0.6921, 0.8738],\n",
      "        [0.1679, 0.3789, 0.4707]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# A random tensor\n",
    "# Each element is sampled uniformly between 0 and 1\n",
    "t_rand = torch.rand(2,3)\n",
    "print(t_rand)\n",
    "print(t_rand.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8937eef",
   "metadata": {},
   "source": [
    "## Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "046ab292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(t_a.dtype)\n",
    "t_a_int32 = t_a.to(torch.int32)\n",
    "print(t_a_int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89622efa",
   "metadata": {},
   "source": [
    "## Change shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326bcd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4821, 0.1011, 0.1499],\n",
      "        [0.7022, 0.2854, 0.4937]])\n",
      "tensor([[0.4821, 0.7022],\n",
      "        [0.1011, 0.2854],\n",
      "        [0.1499, 0.4937]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Transpose\n",
    "t = torch.rand(2,3)\n",
    "t_transpose = torch.transpose(t, 0, 1) # Switch dimension 0 and 1.\n",
    "print(t)\n",
    "print(t_transpose)\n",
    "print(t.shape)\n",
    "print(t_transpose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3687d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 1., 10.],\n",
      "        [ 1.,  1.]])\n",
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 10.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose does not copy the original tensor\n",
    "# It just provides another way of indexing\n",
    "t = torch.ones(2,3)\n",
    "t_transpose = torch.transpose(t, 0, 1)\n",
    "print(t)\n",
    "print(t_transpose)\n",
    "t_transpose[1,1]=10\n",
    "print(t_transpose)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97de232d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape\n",
    "t = torch.ones(2*3)\n",
    "t_reshape = t.reshape(2,3)\n",
    "print(t)\n",
    "print(t_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c6db267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [10.,  1.,  1.]])\n",
      "tensor([ 1.,  1.,  1., 10.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "# Similarly, t.reshape does not make a copy of the original tensor\n",
    "t_reshape[1,0] = 10\n",
    "print(t_reshape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a8ac03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1, 4, 1])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Squeeze: remove dimensions of size 1\n",
    "t = torch.zeros(1,2,1,4,1)\n",
    "t_squeeze = torch.squeeze(t) # This will remove all dimensions of size 1\n",
    "print(t.shape)\n",
    "print(t_squeeze.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a827f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Remove certain dimensions\n",
    "t_squeeze = torch.squeeze(t, 2)\n",
    "print(t_squeeze.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "965aaad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Remove certain dimensions\n",
    "# If the size is not 1, nothing will happpen.\n",
    "t_squeeze = torch.squeeze(t, 3)\n",
    "print(t_squeeze.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e3775",
   "metadata": {},
   "source": [
    "## Mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0c30d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5153, -0.4414],\n",
      "        [-0.1939,  0.4694],\n",
      "        [-0.9414,  0.5997],\n",
      "        [-0.2057,  0.5087],\n",
      "        [ 0.1390, -0.1224]])\n",
      "tensor([[ 0.8590,  0.7056],\n",
      "        [-0.3406, -1.2720],\n",
      "        [-1.1948,  0.0250],\n",
      "        [-0.7627,  1.3969],\n",
      "        [-0.3245,  0.2879]])\n"
     ]
    }
   ],
   "source": [
    "# Setting the random seed to a fixed value ensures that the same sequence of random numbers will be generated every time the code is run.\n",
    "torch.manual_seed(1)\n",
    "# Create a tensor whose entries are in [-1, 1)\n",
    "t1 = 2 * torch.rand(5,2) - 1 \n",
    "# Create a tensor whose entries follow the standard normal distribution\n",
    "t2 = torch.normal(mean=0, std=1, size=(5,2))\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89ff95d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4426, -0.3114],\n",
      "        [ 0.0660, -0.5970],\n",
      "        [ 1.1249,  0.0150],\n",
      "        [ 0.1569,  0.7107],\n",
      "        [-0.0451, -0.0352]])\n"
     ]
    }
   ],
   "source": [
    "# Elementwise product\n",
    "t3 = torch.multiply(t1, t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72d007cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1373,  0.2028])\n",
      "tensor(0.0327)\n"
     ]
    }
   ],
   "source": [
    "# Mean along a given axis\n",
    "print(torch.mean(t1, axis=0))\n",
    "# Mean of all entries\n",
    "print(torch.mean(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbad94d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1312,  0.3860, -0.6267, -1.0096, -0.2943],\n",
      "        [ 0.1647, -0.5310,  0.2434,  0.8035,  0.1980],\n",
      "        [-0.3855, -0.4422,  1.1399,  1.5558,  0.4781],\n",
      "        [ 0.1822, -0.5771,  0.2585,  0.8676,  0.2132],\n",
      "        [ 0.0330,  0.1084, -0.1692, -0.2771, -0.0804]])\n",
      "tensor([[ 0.1312,  0.3860, -0.6267, -1.0096, -0.2943],\n",
      "        [ 0.1647, -0.5310,  0.2434,  0.8035,  0.1980],\n",
      "        [-0.3855, -0.4422,  1.1399,  1.5558,  0.4781],\n",
      "        [ 0.1822, -0.5771,  0.2585,  0.8676,  0.2132],\n",
      "        [ 0.0330,  0.1084, -0.1692, -0.2771, -0.0804]])\n",
      "tensor([[ 1.7453,  0.3392],\n",
      "        [-1.6038, -0.2180]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "t5 = torch.matmul(t1, torch.transpose(t2, 0, 1)) # To transpose t2, we can also use t2.transpose(0, 1)\n",
    "print(t5)\n",
    "# To transpose t2, we can also use t2.transpose(0, 1)\n",
    "t5 = torch.matmul(t1, t2.transpose(0, 1))\n",
    "print(t5)\n",
    "t6 = torch.matmul(t1.transpose(0, 1), t2)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ee3274c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5165)\n",
      "tensor(2.1417)\n",
      "tensor([0.9566, 0.6632, 1.5412, 0.7145, 0.2615])\n"
     ]
    }
   ],
   "source": [
    "# Norms\n",
    "# Frobenius norm (the default norm, the square root of the sum of the squares of the elements)\n",
    "norm_Frobenius_t1 = torch.linalg.norm(t1)\n",
    "print(norm_Frobenius_t1)\n",
    "# L1 norm\n",
    "norm_L1_t1 = torch.linalg.norm(t1, ord=1)\n",
    "print(norm_L1_t1)\n",
    "# L1 norm along a specific dimension\n",
    "norm_L1_t1_dim1 = torch.linalg.norm(t1, ord=1, dim=1)\n",
    "print(norm_L1_t1_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d7074d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius direct calculation tensor(1.5165)\n",
      "Frobenius tensor(1.5165)\n",
      "Sum of absolute values tensor(4.1370)\n",
      "L1 tensor(2.1417)\n"
     ]
    }
   ],
   "source": [
    "# Validation of the norms\n",
    "# Validate the Frobenius norm\n",
    "print(\"Frobenius direct calculation\", torch.sqrt(torch.sum(torch.multiply(t1, t1))))\n",
    "print(\"Frobenius\", norm_Frobenius_t1)\n",
    "\n",
    "# Validate the L1 norm\n",
    "# The L1 norm is not the sum of the absolute values of all elements\n",
    "# The L1 norm is :torch.max(torch.sum(torch.abs(t1), dim=0))\n",
    "# i.e.: max(sum(abs(t1), dim=0))\n",
    "# Namely, we take absolute values of all elements, them add along the first dimension (along each column), then we select the maximum sum as the L1 norm\n",
    "print(\"Sum of absolute values\", torch.sum(torch.abs(t1)))\n",
    "print(\"L1\", norm_L1_t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67dd9e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# Another example of L1 norm\n",
    "t = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "print(t)\n",
    "print(torch.linalg.norm(t, ord=1))\n",
    "# Obviously, the result is not the sum of all absolute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f394d",
   "metadata": {},
   "source": [
    "## Split and join tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "741358ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6397, 0.9743, 0.8300, 0.0444, 0.0246, 0.2588])\n",
      "(tensor([0.6397, 0.9743]), tensor([0.8300, 0.0444]), tensor([0.0246, 0.2588]))\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(6)\n",
    "print(t)\n",
    "# Split into three parts\n",
    "t_splits = torch.chunk(t, 3)\n",
    "print(t_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb3ce5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.6397, 0.9743]), tensor([0.8300, 0.0444]), tensor([0.0246, 0.2588]))\n"
     ]
    }
   ],
   "source": [
    "# When the number of chunks cannot divide the total number of elements, the result can be weird\n",
    "# The following code tries to divide t into 4 parts. However, the result is a list of 3 tensors\n",
    "# As mentioned in the book, the last tensor of the split should be smaller than the others.\n",
    "# If the first three chunks have size 1, then the last will have size 3, which is larger\n",
    "# If the first three chunks have size 2, then the last will have size 0, and this is just the result we see in the following\n",
    "t_splits = torch.chunk(t, 4)\n",
    "print(t_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94febdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9391, 0.4167, 0.7140, 0.2676, 0.9906])\n",
      "(tensor([0.9391, 0.4167, 0.7140]), tensor([0.2676, 0.9906]))\n"
     ]
    }
   ],
   "source": [
    "# Specify split size of each chunk\n",
    "t = torch.rand(5)\n",
    "print(t)\n",
    "t_splits = torch.split(t, split_size_or_sections=[3,2])\n",
    "print(t_splits)\n",
    "# It seems that there is no way to specify the size of each chunk. All chunk sizes must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4b13915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([0., 0.])\n",
      "tensor([1., 1., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate\n",
    "A = torch.ones(3)\n",
    "B = torch.zeros(2)\n",
    "C = torch.cat([A,B], axis=0)\n",
    "print(A)\n",
    "print(B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25257c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Stack\n",
    "A = torch.ones(3)\n",
    "B = torch.zeros(3)\n",
    "C = torch.stack([A,B], axis=1)\n",
    "print(A)\n",
    "print(B)\n",
    "print(C)\n",
    "C = torch.stack([A,B], axis=0)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3098d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation from ChatGPT\n",
    "# torch.cat concatenates a sequence of tensors along an existing dimension\n",
    "# torch.stack concatenates a sequence of tensors along a new dimension\n",
    "# Therefore, C = torch.cat([A,B], axis=1) will not work, since the axis=1 does not exist\n",
    "# But torch.stack([A,B], axis=1) will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16d7b32c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([[4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# In order to concatenate two row vector, we need to define them as 2D tensor\n",
    "E = torch.tensor([[1,2,3]])\n",
    "F = torch.tensor([[4,5,6]])\n",
    "G = torch.cat([E,F], axis=0)\n",
    "print(E)\n",
    "print(F)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8178085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# However, in this situation, torch.stack will not give desired results since it always add a new dimension\n",
    "G = torch.stack([E,F], axis=0)\n",
    "print(G)\n",
    "print(G.shape)\n",
    "G = torch.stack([E,F], axis=1)\n",
    "print(G)\n",
    "print(G.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "434346fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = torch.stack([E,F], axis=0)\n",
    "G[1]==F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06eac38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = torch.stack([E,F], axis=1)\n",
    "G[:,1,:]==F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4d502",
   "metadata": {},
   "source": [
    "Maybe it is better to understand torch.stack from a algebraic view. The shape of E and F is both (1,3). 'torch.stack([E,F], axis=0)' adds a new dimension as the first dimension, so that the result has shape (2,1,3). Thus, G[0]==E, G[1]==F. 'torch.stack([E,F], axis=1)' adds a new dimension as the second dimension, so that the result has shape (1,2,3). In this case, G[:,0,:]==E and G[:,1,:]==F."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
