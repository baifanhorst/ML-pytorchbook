{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b09cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ecec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example z = 2*(a-b) + c\n",
    "# a, b, c can be tensor scalar, 1st order tensor, 2nd order tensor\n",
    "# In the last two cases, the operation is elementwise.\n",
    "def cal_z(a,b,c):\n",
    "    r1 = torch.sub(a,b)\n",
    "    r2 = torch.mul(r1, 2)\n",
    "    z = torch.add(r2, c)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3765413d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar Inputs: tensor(1)\n",
      "Rank 1 Inputs: tensor([1])\n",
      "Rank 1 Inputs: tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "# Try different inputs\n",
    "a = torch.tensor(1)\n",
    "b = torch.tensor(2)\n",
    "c = torch.tensor(3)\n",
    "print('Scalar Inputs:', cal_z(a,b,c))\n",
    "\n",
    "a = torch.tensor([1])\n",
    "b = torch.tensor([2])\n",
    "c = torch.tensor([3])\n",
    "print('Rank 1 Inputs:', cal_z(a,b,c))\n",
    "\n",
    "a = torch.tensor([[1]])\n",
    "b = torch.tensor([[2]])\n",
    "c = torch.tensor([[3]])\n",
    "print('Rank 1 Inputs:', cal_z(a,b,c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bf5f1",
   "metadata": {},
   "source": [
    "## Generate tensors that require gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755ea448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Set requires_grad=True to initialize a tensor with a gradient\n",
    "a = torch.tensor(3.14, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60ff585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f745c708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Without requires_grad=True, no gradient will be stored\n",
    "w = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b4a326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually turn on the option\n",
    "w.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5c95e",
   "metadata": {},
   "source": [
    "## Generate random tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0839aa19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4183,  0.1688,  0.0390],\n",
       "        [ 0.3930, -0.2858, -0.1051]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "# This creates a tensor without initializing its values. \n",
    "# It only allocates memory to store this tensor.\n",
    "# If we print w, we will see highly irregular values\n",
    "w = torch.empty(2,3)\n",
    "# This is a special initialization method\n",
    "# The input w is usually a weight matrix\n",
    "# The intial entries of w follow normal distribution with zero mean\n",
    "# However, the variance is calculated using the Xavier initialization formula. \n",
    "# This initialization method is designed to ensure that \n",
    "# the variance of the activations and gradients of the layer \n",
    "# are approximately the same for all layers in the network, \n",
    "# which can improve the training performance of deep neural networks.\n",
    "torch.nn.init.xavier_normal_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e1ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? Further use?\n",
    "# torch.nn.Module is a base class for all neural network modules\n",
    "# It has a forward method\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Recall that the second dimension corresponds to input\n",
    "        # For w1, the input is a 3D vector\n",
    "        # The output is a 2D vector\n",
    "        self.w1 = torch.empty(2, 3, requires_grad=True)\n",
    "        torch.nn.init.xavier_normal_(self.w1)\n",
    "        # The input for w2 comes from the output of the first layer, which is a 2D vector.\n",
    "        # Thus the 2nd dimension of w2 has two entries\n",
    "        # The output of w1 is a scalar\n",
    "        self.w2 = torch.empty(1, 2, requires_grad=True)\n",
    "        torch.nn.init.xavier_normal_(self.w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38742df",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac056675",
   "metadata": {},
   "source": [
    "An example   \n",
    "$$z = wx + b$$\n",
    "$$Loss = \\sum_{i=1}^{N} (y_i - z_i)^2$$\n",
    "$x$ and $z$ are vetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f1f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get the gradient of the loss wrt w and b\n",
    "# Therefore, we set requires_grad=True when defining w and b\n",
    "w = torch.tensor(1.0, requires_grad=True) \n",
    "b = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "x = torch.tensor([1.4])\n",
    "y = torch.tensor([2.1])\n",
    "\n",
    "z = torch.add(torch.mul(w, x), b)\n",
    "loss = (y-z).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcae3593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(0.5000, requires_grad=True)\n",
      "tensor([1.4000])\n",
      "tensor([2.1000])\n",
      "tensor([1.9000], grad_fn=<AddBackward0>)\n",
      "tensor(0.0400, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6a55fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor(-0.5600)\n",
      "dL/db tensor(-0.4000)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print('dL/dw', w.grad)\n",
    "print('dL/db', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d427c19",
   "metadata": {},
   "source": [
    "Manual verification:   \n",
    "$$\\frac{\\partial L}{\\partial w} = 2(wx+b-y)x = 2(1 \\cdot 1.4 + 0.5 - 2.1)* 1.4=-0.56$$\n",
    "$$\\frac{\\partial L}{\\partial b} = 2(wx+b-y) = 2(1 \\cdot 1.4 + 0.5 - 2.1)=-0.4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c00cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5600], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(2 * x * (w*x+b-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f2b0a",
   "metadata": {},
   "source": [
    "The above result contains 'grad_fn=<MulBackward0>' ,which means that the tensor was created as a result of an operation that was tracked by PyTorch's autograd system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
